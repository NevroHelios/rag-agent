# ğŸ“ RAG Agent: Intelligent University Policy Assistant

<div align="center">

![Python](https://img.shields.io/badge/python-v3.13+-blue.svg)
![LangChain](https://img.shields.io/badge/LangChain-v0.3.23+-green.svg)
![Qdrant](https://img.shields.io/badge/Qdrant-Vector%20DB-red.svg)
![Groq](https://img.shields.io/badge/Groq-LLM-purple.svg)

**An intelligent RAG (Retrieval-Augmented Generation) agent that answers university policy questions using advanced document retrieval and AI reasoning.**

[ğŸš€ Quick Start](#-quick-start) â€¢ [ğŸ“‹ Features](#-features) â€¢ [ğŸ—ï¸ Architecture](#ï¸-architecture) â€¢ [ğŸ› ï¸ Usage](#ï¸-usage) â€¢ [ğŸ“Š Examples](#-examples)

</div>

---

## ğŸŒŸ What is RAG Agent?

RAG Agent is a sophisticated question-answering system that combines the power of **vector databases** and **large language models** to provide accurate, contextual answers about university policies, course requirements, and academic procedures.

### ğŸ¯ Key Capabilities

- ğŸ“š **Document Understanding**: Processes complex academic documents (markdown format)
- ğŸ” **Semantic Search**: Uses advanced embeddings for context-aware retrieval
- ğŸ¤– **Intelligent Reasoning**: Leverages Groq's LLaMA 3 model for nuanced responses
- ğŸ”„ **Hybrid Search**: Combines vector similarity with intelligent filtering
- ğŸ’¬ **Natural Conversation**: Maintains context across multi-turn conversations

---

## ğŸ—ï¸ Architecture Overview

```mermaid
graph TB
    A[ğŸ“„ Academic Documents] --> B[ğŸ”§ Document Processor]
    B --> C[âœ‚ï¸ Text Splitter]
    C --> D[ğŸ§  HuggingFace Embeddings]
    D --> E[(ğŸ—„ï¸ Qdrant Vector Store)]
    
    F[ğŸ‘¤ User Query] --> G[ğŸ¤– RAG Agent]
    G --> H[ğŸ” Vector Retrieval]
    H --> E
    E --> I[ğŸ“Š Relevant Documents]
    I --> J[ğŸ§  Groq LLM]
    J --> K[âœ¨ Generated Answer]
    
    style A fill:#e1f5fe
    style F fill:#f3e5f5
    style K fill:#e8f5e8
    style E fill:#fff3e0
```

### ğŸ”§ Technology Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Vector Database** | ğŸ—„ï¸ Qdrant | Stores and retrieves document embeddings |
| **Embeddings** | ğŸ¤— HuggingFace (`BAAI/bge-small-en-v1.5`) | Converts text to semantic vectors |
| **LLM** | ğŸš€ Groq LLaMA 3 (70B) | Generates intelligent responses |
| **Framework** | ğŸ¦œ LangChain + LangGraph | Orchestrates the RAG pipeline |
| **Text Processing** | ğŸ“ Unstructured | Handles markdown document parsing |

---

## ğŸ“‹ Features

### ğŸ¯ Core Features
- âœ… **Semantic Document Search** - Find relevant policy sections using meaning, not just keywords
- âœ… **Context-Aware Responses** - Understands complex academic scenarios and relationships
- âœ… **Multi-format Support** - Processes structured academic documents (currently Markdown)
- âœ… **Persistent Vector Storage** - Efficient caching and retrieval of document embeddings
- âœ… **Tool-Based Architecture** - Modular design with specialized retrieval tools

### ğŸš€ Advanced Capabilities
- ğŸ”„ **Hybrid Search Strategy** - Combines multiple retrieval techniques
- ğŸ¨ **Rich Console Output** - Colorized debugging and progress tracking
- ğŸ§© **Modular Design** - Easy to extend with new document types or retrieval methods
- ğŸ”§ **Configurable Parameters** - Adjust chunk sizes, embedding models, and retrieval settings

---

## ğŸš€ Quick Start

### ğŸ“‹ Prerequisites

- Python 3.13+
- Qdrant server (local or cloud)
- Groq API key

### ğŸ› ï¸ Installation

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd rag-agent
   ```

2. **Install dependencies**
   ```bash
   pip install -e .
   ```

3. **Set up environment variables**
   ```bash
   # Create .env file
   echo "GROQ_API_KEY=your_groq_api_key_here" > .env
   ```

4. **Start Qdrant server**
   ```bash
   # Using Docker
   docker run -p 6333:6333 qdrant/qdrant
   
   # Or install locally
   # Follow: https://qdrant.tech/documentation/quick-start/
   ```

5. **Run the agent**
   ```bash
   python main.py
   ```

---

## ğŸ› ï¸ Usage

### ğŸ¯ Basic Usage

```python
from main import run_agent

# Ask a question about university policies
answer = run_agent("What are the eligibility criteria for final exams?")
print(answer)
```

### ğŸ”§ Advanced Configuration

```python
from hybrid_search import load_or_create_vectore_store

# Customize the vector store
vector_store = load_or_create_vectore_store()
retriever = vector_store.as_retriever(search_kwargs={"k": 5})  # Get top 5 results

# Query specific topics
results = retriever.invoke("OPPE exam requirements")
```

### ğŸ“Š Document Processing

The system automatically processes your academic documents:

```python
# Documents are processed with these settings:
CHUNK_SIZE = 1000        # Characters per chunk
CHUNK_OVERLAP = 300      # Overlap between chunks
EMBEDDING_MODEL = "BAAI/bge-small-en-v1.5"  # Semantic embeddings
```

---

## ğŸ“Š Examples

### ğŸ“ Academic Queries

**Query**: *"What are the eligibility requirements for OPPE exams?"*

**Response**: 
> Based on the university policies, there are two main OPPE eligibility requirements:
> 
> **For OPPE 1:**
> - Students must complete the OPPE System Compatibility Test (SCT) exam
> - OPPE1 will not be scheduled for students who fail to complete the SCT
> 
> **For OPPE 2:**
> - Most courses have specific eligibility criteria
> - If you don't meet the final exam eligibility, OPPE2 won't be scheduled
> - You would receive a WA/WQ grade and need to repeat the entire course

### ğŸ“š Course Information

**Query**: *"I got low marks in MAD1, should I move to MAD2 or repeat?"*

**Response**:
> Based on the course structure, you should **not** move to MAD2 if you got low marks in MAD1. Here's why:
> 
> - **MAD1 Project is a co-requisite** for MAD2 Theory
> - You need to successfully complete MAD1 before progressing
> - Consider repeating MAD1 to build a solid foundation
> - The diploma pathway shows MAD1 â†’ MAD1 Project â†’ MAD2 Theory â†’ MAD2 Project

---

## ğŸ”§ Configuration

### ğŸ“ Project Structure

```
rag-agent/
â”œâ”€â”€ ğŸ“„ main.py              # Main agent orchestration
â”œâ”€â”€ ğŸ” hybrid_search.py     # Vector store management
â”œâ”€â”€ ğŸ“‹ pyproject.toml       # Project dependencies
â”œâ”€â”€ ğŸ“Š data/
â”‚   â””â”€â”€ gradingdoc.md      # University policy document
â”œâ”€â”€ ğŸ”§ .env                # Environment variables
â””â”€â”€ ğŸ“– README.md           # This file
```

### âš™ï¸ Environment Variables

```bash
# Required
GROQ_API_KEY=your_groq_api_key

# Optional (with defaults)
QDRANT_HOST=localhost
QDRANT_PORT=6333
EMBEDDING_MODEL=BAAI/bge-small-en-v1.5
```

### ğŸ›ï¸ Customizable Parameters

```python
# In hybrid_search.py
CHUNK_SIZE = 1000           # Adjust for longer/shorter contexts
CHUNK_OVERLAP = 300         # Control information continuity
EMBEDDING_DIMENSION = 384   # Match your embedding model
SEARCH_K = 3               # Number of retrieved documents
```

---

## ğŸ› Troubleshooting

### Common Issues

| Issue | Solution |
|-------|----------|
| ğŸ”´ **Qdrant Connection Error** | Ensure Qdrant server is running on `localhost:6333` |
| ğŸŸ¡ **No documents found** | Check if `data/gradingdoc.md` exists and is readable |
| ğŸ”µ **Groq API Error** | Verify your `GROQ_API_KEY` in `.env` file |
| ğŸŸ£ **Memory Issues** | Reduce `CHUNK_SIZE` or limit `search_k` parameter |

### ğŸ” Debug Mode

Enable detailed logging by checking the colorized console output:

- ğŸ”µ **Blue**: Model interactions
- ğŸŸ¢ **Green**: Tool executions  
- ğŸŸ¡ **Yellow**: Final answers
- ğŸ”´ **Red**: Errors
- ğŸŸ£ **Purple**: Decision points

---

## ğŸ¤ Contributing

We welcome contributions! Here are some ways to help:

- ğŸ“ **Documentation**: Improve setup guides and examples
- ğŸ”§ **Features**: Add new document types or retrieval methods
- ğŸ› **Bug Fixes**: Report and fix issues
- ğŸ§ª **Testing**: Add test cases for different scenarios

### ğŸ”„ Development Workflow

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

---

## ğŸ“ˆ Roadmap

### ğŸ¯ Upcoming Features

- [ ] ğŸ“± **Web Interface** - User-friendly web UI
- [ ] ğŸ”— **Multi-Document Support** - Handle multiple policy documents
- [ ] ğŸ“Š **Analytics Dashboard** - Query analytics and performance metrics
- [ ] ğŸŒ **API Endpoints** - RESTful API for integration
- [ ] ğŸ”„ **Real-time Updates** - Auto-sync document changes
- [ ] ğŸ¨ **Custom Themes** - Personalized UI themes

### ğŸš€ Performance Improvements

- [ ] âš¡ **Caching Layer** - Redis-based response caching
- [ ] ğŸ” **Advanced Search** - Hybrid keyword + semantic search
- [ ] ğŸ“ˆ **Scalability** - Multi-instance deployment support

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- ğŸ¤— **HuggingFace** for excellent embedding models
- ğŸš€ **Groq** for lightning-fast LLM inference
- ğŸ—„ï¸ **Qdrant** for robust vector database capabilities
- ğŸ¦œ **LangChain** for the comprehensive AI framework

---

<div align="center">

**Built with â¤ï¸ for the academic community**

[â­ Star this repo](../../stargazers) â€¢ [ğŸ› Report Bug](../../issues) â€¢ [ğŸ’¡ Request Feature](../../issues)

</div>
